{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import mwxml\n",
    "import numpy as np\n",
    "import wikitextparser as wtp\n",
    "\n",
    "from tools.wikiparser_utils import WikiXMLDump, WikiPage\n",
    "from duckduckgo_search import ddg\n",
    "from tqdm import tqdm\n",
    "from difflib import Differ \n",
    "from tools.difflibparser import DifflibParser, DiffCode\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = 'data/documents'\n",
    "PAGES_DIR = 'data/pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_page(page_name):\n",
    "    return bool(re.search('[a-zA-Z]', page_name))\n",
    "\n",
    "def filter_comment(comment_text, user_name):\n",
    "    com_text = comment_text.strip()\n",
    "    if 'bot' in com_text or 'bot' in user_name:\n",
    "        return False\n",
    "    \n",
    "    if com_text[-2:] == '*/':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CHANGE_SYMB_LEN = 5\n",
    "MAX_CHANGE_SYMB_LEN = 300\n",
    "MAX_PAGE_SYMB_LEN = 20000\n",
    "MAX_ABSTRACT_LEN = 800\n",
    "MIN_ABSTRACT_LEN = 10\n",
    "MIN_COMMENT_LEN = 5\n",
    "abstarct_tokenizer = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(l, r, arr):\n",
    "    l_ans, r_ans = -1, -1\n",
    "    for sent_idx, (l_arr, r_arr) in enumerate(zip(arr, arr[1:])):\n",
    "        if l_arr <= l < r_arr:\n",
    "            l_ans = sent_idx\n",
    "        if l_arr <= r < r_arr:\n",
    "            r_ans = sent_idx\n",
    "    if l_ans == -1:\n",
    "        l_ans = len(arr) - 1\n",
    "    if r_ans == -1:\n",
    "        r_ans = len(arr) - 1\n",
    "    return l_ans, r_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('=====', '==').replace('====', '==').replace('===', '==')\n",
    "    text = re.sub('\\[\\[File.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[Category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = re.sub('\\[\\[category:.*?]]', '', text, count=0, flags=0)\n",
    "    text = wtp.remove_markup(text)\n",
    "    text = text.replace('\\t', '').replace('\\n\\n\\n', '\\n\\n').replace('\\n\\n\\n', '\\n\\n')\n",
    "    text = text.replace('\\n\\n*', ', ').replace('\\n\\n*', ', ')\n",
    "    return text\n",
    "    \n",
    "def clean_section_text(text):\n",
    "    text = re.sub('==.*?==+', '', text, count=0, flags=0)\n",
    "    return text.strip()\n",
    "\n",
    "def text2sentences(text, sent_tokenizer=nltk.sent_tokenize):\n",
    "    idxs_arr = []\n",
    "    sents = sent_tokenizer(text)\n",
    "    cur_str = text[:]\n",
    "    cur_skip = 0\n",
    "    idxs2sent = {}\n",
    "    for sent in sents:\n",
    "        match_idx = cur_str.find(sent)\n",
    "        start_idx = match_idx + cur_skip\n",
    "        idxs_arr.append(start_idx)\n",
    "        finish_idx = match_idx + cur_skip + len(sent) - 1\n",
    "        idxs2sent[(start_idx, finish_idx)] = sent\n",
    "        if finish_idx + 1 < len(cur_str):\n",
    "            cur_skip = finish_idx + 1\n",
    "            cur_str = cur_str[match_idx + len(sent):]\n",
    "    return idxs2sent, np.array(sents), idxs_arr\n",
    "\n",
    "def extract_important_sections(text):\n",
    "    parsed_text = wtp.parse(text)\n",
    "    section_titles, section_texts = [], []\n",
    "    for sec in parsed_text.sections:\n",
    "        if not sec.title:\n",
    "            #for par in sec.string.split('\\n\\n'):\n",
    "            section_titles.append(sec.title)\n",
    "            section_texts.append(clean_section_text(sec.string))\n",
    "            continue\n",
    "        if 'external links' in sec.title.lower():\n",
    "            continue\n",
    "        if 'references' in sec.title.lower():\n",
    "            continue\n",
    "        if 'notes' in sec.title.lower():\n",
    "            continue\n",
    "        if 'see also' in sec.title.lower():\n",
    "            continue\n",
    "        \n",
    "        #for par in sec.string.split('\\n\\n'):\n",
    "        section_titles.append(sec.title)\n",
    "        section_texts.append(clean_section_text(sec.string))\n",
    "    return section_titles, section_texts\n",
    "\n",
    "def get_diff_num2(prev_sections_texts, new_sections_texts):\n",
    "    differ_obj = Differ()\n",
    "    dif_result = list(DifflibParser(prev_sections_texts, new_sections_texts))\n",
    "    result = []\n",
    "    result_idxs = []\n",
    "    old_text, new_text, last_diff_id = [], [], -1000\n",
    "    for dif_id, dif_line in enumerate(dif_result):\n",
    "        if dif_line['code'] != DiffCode.SIMILAR:\n",
    "            if np.abs(dif_id - last_diff_id) > 0:\n",
    "                result.append(dif_line)\n",
    "                result_idxs.append(dif_id)\n",
    "                last_diff_id = dif_id\n",
    "    return result_idxs, result    \n",
    "\n",
    "def get_changes(diffs):\n",
    "    all_changes = []\n",
    "    all_changes_sents = []\n",
    "    for diff_id, diff_obj in enumerate(diffs):\n",
    "        if diff_obj['code'] == DiffCode.RIGHTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'r'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.LEFTONLY:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) < MIN_ABSTRACT_LEN:\n",
    "                continue\n",
    "            all_changes.append(([diff_obj['line']], 'l'))\n",
    "            _, sents, _ = text2sentences(diff_obj['line'])\n",
    "            all_changes_sents.append(sents)\n",
    "            \n",
    "        elif diff_obj['code'] == DiffCode.CHANGED:\n",
    "            if len(abstarct_tokenizer(diff_obj['line'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            if len(abstarct_tokenizer(diff_obj['newline'])) > MAX_ABSTRACT_LEN:\n",
    "                continue\n",
    "            idxs2sent, sents, idxs_arr = text2sentences(diff_obj['newline'])\n",
    "            all_changes_sents = []\n",
    "            r_change = diff_obj['rightchanges']\n",
    "            cur_ch = -10\n",
    "            prev_ch = -10\n",
    "            all_r_changes = []\n",
    "            changed_sents = []\n",
    "            for ch in r_change:\n",
    "                if prev_ch < 0:\n",
    "                    prev_ch = ch\n",
    "                    cur_ch = ch\n",
    "                if np.abs(ch - cur_ch) > 1:\n",
    "                    new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "                    if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                        all_r_changes.append(new_change)\n",
    "                        sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                        changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "                    prev_ch = ch\n",
    "                cur_ch = ch\n",
    "            new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "            if new_change.strip() != '' and len(new_change.strip()) > MIN_CHANGE_SYMB_LEN:\n",
    "                all_r_changes.append(new_change)\n",
    "                sents_idxs_l, sents_idxs_r = find_nearest(prev_ch, cur_ch+1, idxs_arr)\n",
    "                changed_sents += list(range(sents_idxs_l, sents_idxs_r+1))\n",
    "            all_changes.append((all_r_changes, 'c'))\n",
    "            changed_sents = sorted(list(set(changed_sents)))\n",
    "            all_changes_sents.append(sents[changed_sents])\n",
    "    return all_changes, all_changes_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_dump.xml']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DUMPS = os.listdir('data/dump')\n",
    "DUMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "total_counter = 0\n",
    "\n",
    "for dump_name in DUMPS:\n",
    "    dump = mwxml.Dump.from_file(open(f'dump/{dump_name}', encoding=\"utf-8\"))\n",
    "    pbar = tqdm(position=0, leave=True)\n",
    "\n",
    "    for page in dump:\n",
    "        if not filter_page(page.title):\n",
    "            continue\n",
    "        revisions = []\n",
    "        for rev in page:\n",
    "            revisions.append(rev)\n",
    "\n",
    "        if len(revisions) < 2: \n",
    "            continue\n",
    "        good_revisions = []\n",
    "\n",
    "        last_added = len(revisions)\n",
    "        for cur_rev_id in range(len(revisions) - 1, 1, -1):\n",
    "            if cur_rev_id >= last_added:\n",
    "                continue\n",
    "            cur_rev = revisions[cur_rev_id]\n",
    "            if cur_rev.text:\n",
    "                cur_rev_text = cur_rev.text # clean_text(cur_rev.text)\n",
    "            else:\n",
    "                cur_rev_text = ''\n",
    "\n",
    "            for new_rev_id in range(cur_rev_id, 0, -1):\n",
    "                new_rev = revisions[new_rev_id]\n",
    "\n",
    "                if new_rev.text:\n",
    "                    new_rev_text = new_rev.text # clean_text(new_rev.text)\n",
    "                else:\n",
    "                    new_rev_text = ''\n",
    "\n",
    "                if cur_rev_text == new_rev_text:\n",
    "                    last_added = new_rev_id\n",
    "\n",
    "            add_rev = revisions[last_added]\n",
    "            user = ''\n",
    "            if add_rev.user:\n",
    "                if add_rev.user.text:\n",
    "                    user = add_rev.user.text.lower()\n",
    "            revision_dict = {\n",
    "                'text': cur_rev_text,\n",
    "                'comment': add_rev.comment,\n",
    "                'id': add_rev.id,\n",
    "                'page_name': page.title,\n",
    "                'user_name': user\n",
    "            }\n",
    "            good_revisions.append(revision_dict)\n",
    "        good_revisions = good_revisions[::-1]\n",
    "\n",
    "        for prev_rev, new_rev in zip(good_revisions[:], good_revisions[1:]):\n",
    "            total_counter += 1\n",
    "            comment = new_rev['comment']\n",
    "            if comment and len(comment.strip()) > MIN_COMMENT_LEN:\n",
    "                if filter_comment(comment, new_rev['user_name']):\n",
    "                    if np.abs(len(new_rev['text']) - len(prev_rev['text'])) < MAX_CHANGE_SYMB_LEN:\n",
    "                        prev_section_titles, prev_section_texts = extract_important_sections(clean_text(prev_rev['text']))\n",
    "                        new_section_titles, new_section_texts = extract_important_sections(clean_text(new_rev['text']))\n",
    "\n",
    "                        r_idx, r = get_diff_num2(prev_section_texts, new_section_texts)\n",
    "                        if len(r) == 1 and 'newline' in r[0]:\n",
    "                            section_name = ''\n",
    "                            try:\n",
    "                                section_name_t = new_section_titles[r_idx[0]]\n",
    "                                if section_name_t:\n",
    "                                    section_name = section_name_t\n",
    "                            except:\n",
    "                                pass\n",
    "                            ts = new_rev['page_name'] + ' ' + section_name\n",
    "\n",
    "                            all_changes_r, all_changes_sents_r = get_changes(r)\n",
    "                            if len(all_changes_sents_r) > 0 and len(all_changes_sents_r[0]) > 0:\n",
    "                                ts = new_rev['page_name'] + ' ' + section_name                                    \n",
    "\n",
    "                                final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                if os.path.exists(final_page_path) and os.path.exists(final_docs_path):\n",
    "                                    counter += 1\n",
    "                                    continue\n",
    "\n",
    "                                downloaded_docs = []\n",
    "                                search_queries_list = []\n",
    "                                q2docs_num = []\n",
    "                                for ch_text_idx, ch_text in enumerate(all_changes_r[0][0]):\n",
    "                                        fq = ts.strip() + ' ' + ch_text\n",
    "                                        # print(f'Final search query {ch_text_idx}:\\t', fq)\n",
    "                                        search_queries_list.append(fq)\n",
    "                                        search_result = ddg(fq)\n",
    "                                        counter_found_docs = 0\n",
    "                                        if search_result is not None:\n",
    "                                            for search_result_obj in search_result:\n",
    "                                                downloaded_docs.append(search_result_obj['body'])\n",
    "                                                counter_found_docs += 1\n",
    "                                        q2docs_num.append(counter_found_docs)\n",
    "\n",
    "                                json_obj = {\n",
    "                                    \"old_text\": r[0]['line'],\n",
    "                                    \"new_text\": r[0]['newline'],\n",
    "                                    \"title\": new_rev['page_name'],\n",
    "                                    \"comment\": comment,\n",
    "                                    \"section_name\": section_name,\n",
    "                                    \"search_queries\": search_queries_list,\n",
    "                                    \"counter_found_docs\": q2docs_num,\n",
    "                                    \"change_texts\": all_changes_r\n",
    "                                }\n",
    "\n",
    "                                final_page_path = f\"{PAGES_DIR}/{counter}.json\"\n",
    "                                with open(final_page_path, 'w', encoding='utf-8') as f:\n",
    "                                    json.dump(json_obj, f)\n",
    "                                # changed_text = [ctxt for ctxt in all_changes_r[0][0]]\n",
    "                                # changed_text_full = ' '.join(changed_text)\n",
    "\n",
    "                                final_docs_path = f\"{DOCS_DIR}/{counter}.txt\"\n",
    "                                with open(final_docs_path, 'w', encoding='utf-8') as f:\n",
    "                                    for doc_text_idx, doc_text in enumerate(downloaded_docs):\n",
    "                                        f.write(doc_text)\n",
    "                                        if doc_text_idx != len(downloaded_docs) - 1:\n",
    "                                            f.write(\"\\nNEW_DOC\\n\")\n",
    "\n",
    "                                counter += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(downloaded_docs=counter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
