{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подсчет статитстики по выгруженным документам. Смотрим на ревизии подряд. Смотрим только добавленные документы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiparser_utils import WikiXMLDump, WikiPage\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from difflib import Differ \n",
    "from utils.difflibparser import DifflibParser, DiffCode\n",
    "import html2text\n",
    "import os\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = WikiXMLDump('data/history.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = 'data/documents'\n",
    "DOCS_MAPPER_DIR = 'data/documents_mapper'\n",
    "\n",
    "if not os.path.exists(DOCS_MAPPER_DIR):\n",
    "    os.makedirs(DOCS_MAPPER_DIR)\n",
    "    \n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    os.makedirs(DOCS_DIR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_diff(links_old, links_new):\n",
    "    '''\n",
    "    Возращает добавленные ссылки\n",
    "    '''\n",
    "    set_old = set(links_old)\n",
    "    set_new = set(links_new)\n",
    "    set_diff = set_new - set_old\n",
    "    set_diff2 = set_old - set_new\n",
    "    return list(set_diff)\n",
    "\n",
    "def text_diff(old_rev, new_rev, sentence_tokenizer=nltk.sent_tokenize, epsilon=2, beta=2*2):\n",
    "    '''\n",
    "    Бьет текст страницы на предложения (с помощью sentence_tokenizer)\n",
    "    Берем абзацы из epsilon * 2 + 1 предложений\n",
    "    Смотрим различия в абзацах. Возможна ситуация, что diff-абзацы пересекаются, на этот случай есть параметр beta\n",
    "    Следующий diff абзац не раньше чем через beta абзацей\n",
    "    '''\n",
    "    d = Differ()\n",
    "    old_text = old_rev.get_plain_text()\n",
    "    new_text = new_rev.get_plain_text()\n",
    "    \n",
    "    sent_text_old = sentence_tokenizer(old_text)\n",
    "    sent_text_new = sentence_tokenizer(new_text)\n",
    "    \n",
    "    old_sents = []\n",
    "    for i in range(epsilon, len(sent_text_old) - epsilon):\n",
    "        old_sents.append(' '.join(sent_text_old[(i-epsilon):(i+epsilon+1)]))\n",
    "    if not old_sents:\n",
    "        old_sents = [' '.join(sent_text_old)]\n",
    "        \n",
    "    new_sents = []\n",
    "    for i in range(epsilon, len(sent_text_new) - epsilon):\n",
    "        new_sents.append(' '.join(sent_text_new[(i-epsilon):(i+epsilon+1)]))\n",
    "    if not new_sents:\n",
    "        new_sents = [' '.join(sent_text_new)]\n",
    "        \n",
    "    \n",
    "    result = []\n",
    "    dif_result = list(DifflibParser(old_sents, new_sents))\n",
    "    old_text, new_text, last_diff_id = [], [], -1000\n",
    "    for dif_id, dif_line in enumerate(dif_result):\n",
    "        if dif_line['code'] != DiffCode.SIMILAR:\n",
    "            if np.abs(dif_id - last_diff_id) > beta:\n",
    "                result.append(dif_line)  \n",
    "                last_diff_id = dif_id\n",
    "    return result\n",
    "\n",
    "def get_changes(diffs):\n",
    "    \"\"\"\n",
    "    Извлекаем текст различия абзацей\n",
    "    \"\"\"\n",
    "    all_changes = {}\n",
    "    for diff_id, diff_obj in enumerate(diffs):\n",
    "        if diff_obj['code'] == DiffCode.RIGHTONLY:\n",
    "            all_changes[diff_id] = ([diff_obj['line']], 'r')\n",
    "        elif diff_obj['code'] == DiffCode.LEFTONLY:\n",
    "            all_changes[diff_id] = ([diff_obj['line']], 'l')\n",
    "        elif diff_obj['code'] == DiffCode.CHANGED:\n",
    "            r_change = diff_obj['rightchanges']\n",
    "            cur_ch = -10\n",
    "            prev_ch = -10\n",
    "            all_r_changes = []\n",
    "            for ch in r_change:\n",
    "                if prev_ch < 0:\n",
    "                    prev_ch = ch\n",
    "                    cur_ch = ch\n",
    "                if np.abs(ch - cur_ch) > 1:\n",
    "                    new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "                    all_r_changes.append(new_change)\n",
    "                    prev_ch = ch\n",
    "                cur_ch = ch\n",
    "            new_change = diff_obj['newline'][prev_ch:cur_ch+1]\n",
    "            all_r_changes.append(new_change)\n",
    "            all_changes[diff_id] = (all_r_changes, 'c')\n",
    "    return all_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(os.listdir('data/pages'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40d7254af264940bc7f01f347c103ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Statistics processing:   0%|          | 0/925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STATISTICS = {}\n",
    "pbar = tqdm(dirs, total=len(dirs), desc='Statistics processing', leave=True, position=0)\n",
    "for page_name_idx, page_name in enumerate(pbar):\n",
    "\n",
    "    page_revisions = WikiPage().load_revisions(f\"data/pages/{page_name}\")\n",
    "    rev_list = sorted(list(map(int, page_revisions)))\n",
    "    \n",
    "    STATISTICS[page_name] = {}    \n",
    "    STATISTICS[page_name]['num of revisions'] = len(rev_list)\n",
    "    \n",
    "    page_docs_mapper_id2link = json.load(open(f'{DOCS_MAPPER_DIR}/{page_name}/id2link.json', \"r\"))\n",
    "    page_docs_mapper_link2id = json.load(open(f'{DOCS_MAPPER_DIR}/{page_name}/link2id.json', \"r\"))\n",
    "    STATISTICS[page_name]['num of document links'] = len(page_docs_mapper_link2id)\n",
    "    \n",
    "    STATISTICS[page_name]['diff info'] = {}\n",
    "    \n",
    "    for idx, (prev_rev_num, new_rev_num) in enumerate(zip(rev_list, rev_list[1:])):\n",
    "        new_rev = page_revisions[new_rev_num]\n",
    "        prev_rev = page_revisions[prev_rev_num]\n",
    "        comment = new_rev.comment\n",
    "            \n",
    "        new_links, _ = new_rev.get_links()\n",
    "        prev_links, _ = prev_rev.get_links()\n",
    "        diff_links = links_diff(prev_links, new_links)\n",
    "        \n",
    "        STATISTICS[page_name]['diff info'][idx] = {}\n",
    "        comment_exists = 1 if comment else 0\n",
    "        STATISTICS[page_name]['diff info'][idx]['comment exists'] = comment_exists\n",
    "        \n",
    "        num_of_docs = [1 for name_link, url_link in diff_links if url_link in page_docs_mapper_link2id]\n",
    "        STATISTICS[page_name]['diff info'][idx]['num of docs'] = len(num_of_docs)\n",
    "        \n",
    "        \n",
    "        rev_diff = text_diff(prev_rev, new_rev)\n",
    "        STATISTICS[page_name]['diff info'][idx]['num of diff abstracts'] = len(rev_diff)\n",
    "        text_changes = get_changes(rev_diff)\n",
    "        \n",
    "        docs_has_changes_flags = [0 for i in range(len(diff_links))]\n",
    "        change_in_doc_flags = [0 for i in range(len(text_changes))]\n",
    "        \n",
    "        any_docs_has_changes_flags = [0 for i in range(len(diff_links))]\n",
    "        any_change_in_doc_flags = [0 for i in range(len(text_changes))]\n",
    "        \n",
    "        for diff_doc_id, (_, url_link) in enumerate(diff_links):\n",
    "            if url_link not in page_docs_mapper_link2id:\n",
    "                continue\n",
    "            link_id = int(page_docs_mapper_link2id[url_link])\n",
    "            file_text = ''\n",
    "            if os.path.exists(f'{DOCS_DIR}/{page_name}/{link_id}.txt'):\n",
    "                with open(f'{DOCS_DIR}/{page_name}/{link_id}.txt', 'r', encoding='utf-8') as f:\n",
    "                    file_text = f.read()\n",
    "                                \n",
    "            for ch_num, (_, change_info) in enumerate(text_changes.items()):\n",
    "                ALL_CHANGES_FOUND = 1\n",
    "                ANY_CHANGES_FOUND = 0\n",
    "                change_arr_text, change_status = change_info\n",
    "                for change_text in change_arr_text:\n",
    "                    if change_text not in file_text:\n",
    "                        ALL_CHANGES_FOUND = 0\n",
    "                    else:\n",
    "                        ANY_CHANGES_FOUND = 1\n",
    "                        \n",
    "                docs_has_changes_flags[diff_doc_id] = ALL_CHANGES_FOUND\n",
    "                change_in_doc_flags[ch_num] = ALL_CHANGES_FOUND\n",
    "                \n",
    "                any_docs_has_changes_flags[diff_doc_id] = ANY_CHANGES_FOUND\n",
    "                any_change_in_doc_flags[ch_num] = ANY_CHANGES_FOUND\n",
    "                    \n",
    "        STATISTICS[page_name]['diff info'][idx]['ALL num of indep good diffs'] = sum(change_in_doc_flags)\n",
    "        STATISTICS[page_name]['diff info'][idx]['ALL num of indep good docs'] = sum(docs_has_changes_flags)\n",
    "        STATISTICS[page_name]['diff info'][idx]['ALL is perfect'] = 0\n",
    "        if sum(change_in_doc_flags) == len(change_in_doc_flags):\n",
    "            STATISTICS[page_name]['diff info'][idx]['ALL is perfect'] = 1\n",
    "            \n",
    "        STATISTICS[page_name]['diff info'][idx]['ANY num of indep good diffs'] = sum(any_change_in_doc_flags)\n",
    "        STATISTICS[page_name]['diff info'][idx]['ANY num of indep good docs'] = sum(any_docs_has_changes_flags)\n",
    "        STATISTICS[page_name]['diff info'][idx]['ANY is perfect'] = 0\n",
    "        if sum(any_change_in_doc_flags) == len(any_change_in_doc_flags):\n",
    "            STATISTICS[page_name]['diff info'][idx]['ANY is perfect'] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STATISTICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2page = []\n",
    "num_rev2page = []\n",
    "docs2diff = []\n",
    "diffs2dif = []\n",
    "comment_exists = []\n",
    "all_diff = []\n",
    "\n",
    "all_perf2diff = []\n",
    "any_perf2diff = []\n",
    "normal = []\n",
    "super_good = []\n",
    "\n",
    "for pn in STATISTICS:\n",
    "    docs2page.append(STATISTICS[pn]['num of document links'])\n",
    "    num_rev2page.append(STATISTICS[pn]['num of revisions'])\n",
    "    \n",
    "    \n",
    "    COMMENT_EXISTS = 0\n",
    "    NUM_OF_DOCS = 0\n",
    "    NUM_OF_DIFF_ABSTRACT = 0\n",
    "    ALL_GOOD = 0\n",
    "    ANY_GOOD = 0\n",
    "    NORMAL = 0\n",
    "    SUPER = 0\n",
    "    for _, val in STATISTICS[pn]['diff info'].items():\n",
    "        COMMENT_EXISTS += val['comment exists']\n",
    "        NUM_OF_DOCS += val['num of docs']\n",
    "        NUM_OF_DIFF_ABSTRACT += val['num of diff abstracts']\n",
    "        ALL_GOOD += val['ALL is perfect']\n",
    "        ANY_GOOD += val['ANY is perfect']\n",
    "        if val['comment exists'] and val['ANY is perfect']:\n",
    "            NORMAL += 1\n",
    "        if val['comment exists'] and val['ALL is perfect']:\n",
    "            SUPER += 1\n",
    "            \n",
    "        all_diff.append(1)\n",
    "    \n",
    "    normal.append(NORMAL)\n",
    "    super_good.append(SUPER)\n",
    "    docs2diff.append(NUM_OF_DOCS)\n",
    "    diffs2dif.append(NUM_OF_DIFF_ABSTRACT)\n",
    "    comment_exists.append(COMMENT_EXISTS)\n",
    "    all_perf2diff.append(ALL_GOOD)\n",
    "    any_perf2diff.append(ANY_GOOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCS PER PAGE:  1787 / 925 = 1.931891891891892\n",
      "REVS PER PAGE:  69286 / 925 = 74.90378378378378\n",
      "\n",
      "DOCS PER DIFF:  2458 / 68361 = 0.035956173841810386\n",
      "DIFFS PER DIFF:  88661 / 68361 = 1.2969529410043739\n",
      "COMMS PER DIFF:  53065 / 68361 = 0.7762466903643891\n",
      "ALL PER DIFF:  15282 / 68361 = 0.2235485145038838\n",
      "ANY PER DIFF:  15350 / 68361 = 0.22454323371512996\n",
      "NORMAL DIFF:  11117 / 68361 = 0.16262196281505537\n",
      "SUPER DIFF:  11057 / 68361 = 0.16174426939336756\n"
     ]
    }
   ],
   "source": [
    "print('DOCS PER PAGE: ', sum(docs2page), '/', len(STATISTICS), '=', sum(docs2page) / len(STATISTICS))\n",
    "print('REVS PER PAGE: ', sum(num_rev2page), '/', len(STATISTICS), '=', sum(num_rev2page) / len(STATISTICS))\n",
    "print()\n",
    "print('DOCS PER DIFF: ', sum(docs2diff), '/', sum(all_diff), '=', sum(docs2diff) / sum(all_diff))\n",
    "print('DIFFS PER DIFF: ', sum(diffs2dif), '/', sum(all_diff), '=', sum(diffs2dif) / sum(all_diff))\n",
    "print('COMMS PER DIFF: ', sum(comment_exists), '/', sum(all_diff), '=', sum(comment_exists) / sum(all_diff))\n",
    "print('ALL PER DIFF: ', sum(all_perf2diff), '/', sum(all_diff), '=', sum(all_perf2diff) / sum(all_diff))\n",
    "print('ANY PER DIFF: ', sum(any_perf2diff), '/', sum(all_diff), '=', sum(any_perf2diff) / sum(all_diff))\n",
    "print('NORMAL DIFF: ', sum(normal), '/', sum(all_diff), '=', sum(normal) / sum(all_diff))\n",
    "print('SUPER DIFF: ', sum(super_good), '/', sum(all_diff), '=', sum(super_good) / sum(all_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
