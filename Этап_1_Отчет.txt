Сделано: 

- Проверка существующих WIKI датасетов
  Подходящих под нужды моей задачи нет. Мне нужно не только комментарий правки, но и весь текст каждой их резвизий. Я просмотрел более 10 различных
готовых датасетов, однако подходящего так и не нашел. Данные пришлось парсить самостоятельно

- Парсинг дампов
  Познакомился со структурой дампов wikipedia. Также изучил различные инструменты для их парсинга. Что-то пришлось писать самому, однако основной 
инструментарий есть в открытом доступе. 

- Извлечение документов
  Помимо text diff двух ревизий, необходимы документы на основе которых те или иные правки были сделаны. Научился выгружать (парсить) внешние сайты,
на которые ссылается та или иная статья. Кроме этого экспериментировал с внутренними ссылками, однако первичный просмотр показал что они не содержательны,
поэтому остановился только на внешних документах. 

- Подготовка датасета
  Последовательно спарсить и извлечь документы для большого количества дампов. В процессе (так как занимает много времени и места на диске). 
  Пример выгруденных данных для одного из дампов: https://drive.google.com/file/d/1IfYCW5V18N7Yr9eJ-i2s0XsG_rTbwd0d/view?usp=sharing
  
- Датасет с кодом
  Размышления насчет того, что использовать в качестве документов (идея: документация библиотек, либо похожие проекты). 
  Изучение способов парсинга GitHub (запросил API, жду ответ)
  
 
Итог: пока что все идет по плану. Сейчас необходимо провести разведочный анализ данных и провести фильтрацию (не каждая правка содержит новый документ, 
не каждая правка содержит нормальный комментарий). Задача подсчитать какой процент данных (Wiki) теряется из-за недостатка той или иной информации и 
преступить к реализации модели. 
